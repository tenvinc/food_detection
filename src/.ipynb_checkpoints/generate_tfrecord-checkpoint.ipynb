{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating dataset for Tensorflow object detection\n",
    "\n",
    "## Reading in class mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: /home/vencintgamer_gmail_com/food_detection/src/..\n",
      "Data: /home/vencintgamer_gmail_com/food_detection/src/../uec-food/UECFOOD23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "root_path = os.path.join(cwd, os.pardir)\n",
    "dataset_path = os.path.join(root_path, 'uec-food', 'UECFOOD23')\n",
    "\n",
    "print('Root: {}'.format(root_path))\n",
    "print('Data: {}'.format(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class map size: 23\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "category_df = pd.read_csv(os.path.join(dataset_path, \"category.txt\"), sep='\\t')\n",
    "id_to_classes = {int(row['id']): row['name'].strip() for idx, row in category_df.iterrows()}\n",
    "classes = [[v, k] for k, v in id_to_classes.items()] \n",
    "\n",
    "print('Class map size: {}'.format(len(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a data agnostic object to store image info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath: 10, xmin: 10, ymin: 10, xmax: 10, ymax:10, class_id: 10\n"
     ]
    }
   ],
   "source": [
    "class ImageInfo:\n",
    "    def __init__(self, filepath, xmin, ymin, xmax, ymax, class_id):\n",
    "        self.filepath = filepath\n",
    "        self.xmin = xmin\n",
    "        self.ymin = ymin\n",
    "        self.xmax = xmax\n",
    "        self.ymax = ymax\n",
    "        self.class_id = class_id\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"filepath: {}, xmin: {}, ymin: {}, xmax: {}, ymax:{}, class_id: {}\".format(\n",
    "            self.filepath, self.xmin, self.ymin, self.xmax,\n",
    "            self.ymax, self.class_id)\n",
    "\n",
    "img = ImageInfo(10, 10, 10, 10, 10, 10)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in bounding box and label data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dict = {}\n",
    "\n",
    "category_folders = [x[0] for x in os.walk(dataset_path) if os.path.isdir(x[0])]\n",
    "category_folders = category_folders[1:]\n",
    "annotations = []\n",
    "for folder in category_folders:\n",
    "  bbox_df = pd.read_csv(os.path.join(folder, \"bb_info.txt\"), sep=' ')\n",
    "  for _, row in bbox_df.iterrows():\n",
    "    filepath = os.path.join(os.path.basename(folder), '{}'.format(row['img'])) + '.jpg'\n",
    "    info = ImageInfo(filepath, row['x1'], row['y1'], row['x2'], row['y2'],\n",
    "                    int(os.path.basename(folder)))\n",
    "    key = str(row['img'])\n",
    "    if key in img_dict:\n",
    "        img_dict[key].append(info)\n",
    "    else:\n",
    "        img_dict[key] = [info]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle dataset and split into train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: (4183,)\n",
      "Train: (3347,)\n",
      "Val: (836,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_split = 0.8\n",
    "data_size = len(img_dict)\n",
    "rand_idx = np.arange(data_size)\n",
    "np.random.shuffle(rand_idx)\n",
    "\n",
    "key_list = [*img_dict.keys()]\n",
    "np_key = np.asarray(key_list)[rand_idx]\n",
    "\n",
    "np_train_key = np_key[:math.ceil(train_split * data_size)]\n",
    "np_val_key = np_key[math.ceil(train_split * data_size):]\n",
    "\n",
    "# sanity check\n",
    "print('All: {}'.format(np_key.shape))\n",
    "print('Train: {}'.format(np_train_key.shape))\n",
    "print('Val: {}'.format(np_val_key.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialize dataset in memory to TFrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from object_detection.utils import dataset_util\n",
    "\n",
    "def create_tf_example(group, filename):\n",
    "#     print(os.path.join(dataset_path, filename))\n",
    "    with tf.gfile.GFile(os.path.join(dataset_path, '{}'.format(group[0].filepath)), 'rb') as fd:\n",
    "        encoded_jpg = fd.read()\n",
    "    \n",
    "    # Do a read back to check that image encoding is correct\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "    width, height = image.size\n",
    "    \n",
    "#     print(width, height)\n",
    "#     imgplot = plt.imshow(np.asarray(image))\n",
    "#     plt.show()\n",
    "    filename = filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for item in group:\n",
    "        if item.xmax >= width:\n",
    "#             print(item.filepath)\n",
    "            item.xmax = width - 1\n",
    "        if item.ymax >= height:\n",
    "#             print(item.filepath)\n",
    "            item.ymax = height - 1\n",
    "        xmins.append(item.xmin / width)\n",
    "        xmaxs.append(item.xmax / width)\n",
    "        ymins.append(item.ymin / height)\n",
    "        ymaxs.append(item.ymax / height)\n",
    "        classes_text.append(id_to_classes[int(item.class_id)].encode('utf8'))\n",
    "        classes.append(int(item.class_id))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib2\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from progressbar import ProgressBar\n",
    "import math\n",
    "\n",
    "def writeToTFRecordShards(num_per_shard, label, img_dict, keys_arr):\n",
    "    num_shards = math.ceil(len(keys_arr) / num_per_shard)\n",
    "    print(\"Num shards: {}\".format(num_shards))\n",
    "    pbar = ProgressBar()\n",
    "    \n",
    "    # variable to keep track of which shard to put in\n",
    "    curr_idx = 0 \n",
    "    filename = label % (curr_idx, num_shards)\n",
    "    writer = tf.python_io.TFRecordWriter(filename)\n",
    "    for index, key in enumerate(pbar(keys_arr)):\n",
    "            value = img_dict[key]\n",
    "            # sequential sharding due to sequential serializing of tfrecords\n",
    "            if not curr_idx == index // num_per_shard:\n",
    "                writer.close() # close previous writer\n",
    "                curr_idx =  index // num_per_shard\n",
    "                print(\"Changing to {} at index {}\".format(curr_idx, index))\n",
    "                filename = label % (curr_idx, num_shards)\n",
    "                writer = tf.python_io.TFRecordWriter(filename) #start a new one\n",
    "            tf_example = create_tf_example(value, key)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% (170 of 3347) |#                    | Elapsed Time: 0:00:00 ETA:   0:00:02"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start conversion of train dataset...\n",
      "Num shards: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38% (1272 of 3347) |#######             | Elapsed Time: 0:00:01 ETA:   0:00:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing to 1 at index 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68% (2288 of 3347) |#############       | Elapsed Time: 0:00:01 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing to 2 at index 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96% (3220 of 3347) |################### | Elapsed Time: 0:00:02 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing to 3 at index 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (3347 of 3347) |####################| Elapsed Time: 0:00:02 Time:  0:00:02\n",
      " 27% (233 of 836) |######                | Elapsed Time: 0:00:00 ETA:   0:00:00"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start conversion of validation dataset...\n",
      "Num shards: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (836 of 836) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "num_per_shard = 1000\n",
    "train_tf_filebase = os.path.join(root_path, 'data', 'train_dataset-%04d-%04d.record')\n",
    "val_tf_filebase = os.path.join(root_path, 'data', 'val_dataset-%04d-%04d.record')\n",
    "\n",
    "print(\"Start conversion of train dataset...\")\n",
    "writeToTFRecordShards(num_per_shard, train_tf_filebase, img_dict, np_train_key)\n",
    "print(\"Start conversion of validation dataset...\")\n",
    "writeToTFRecordShards(num_per_shard, val_tf_filebase, img_dict, np_val_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (23 of 23) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "from progressbar import ProgressBar\n",
    "\n",
    "label_fname = os.path.join(root_path, 'data', 'labelmap.pbtxt')\n",
    "end = '\\n'\n",
    "s = ' '\n",
    "pbar = ProgressBar()\n",
    "\n",
    "for _, (id, name) in enumerate(pbar(id_to_classes.items())):\n",
    "    # Skip dummy\n",
    "    if id == 0:\n",
    "        continue\n",
    "    out = ''\n",
    "    out += 'item' + s + '{' + end\n",
    "    out += s*2 + 'name:' + ' ' + '\\'' + name + '\\'' + end\n",
    "    out += s*2 + 'id:' + ' ' + (str(id)) + end\n",
    "    out += '}' + end*2\n",
    "    \n",
    "    with open(label_fname, 'a') as f:\n",
    "        f.write(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[filepath: 1/19.jpg, xmin: 352, ymin: 135, xmax: 499, ymax:326, class_id: 1,\n",
       " filepath: 21/19.jpg, xmin: 181, ymin: 183, xmax: 305, ymax:300, class_id: 21]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_gpu] *",
   "language": "python",
   "name": "conda-env-tensorflow_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
